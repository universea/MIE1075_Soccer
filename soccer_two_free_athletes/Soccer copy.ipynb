{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations for completing the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program!  In this notebook, you will learn how to control agents in a more challenging environment, where the goal is to train a team of agents to play soccer.  **Note that this exercise is optional!**\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "import numpy as np\n",
    "from mlagents_envs.base_env import ActionTuple, BaseEnv, DecisionSteps, TerminalSteps\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "\n",
    "from soccer_gym import TransUnity2Gym\n",
    "engine_channel = EngineConfigurationChannel()\n",
    "\n",
    "engine_channel.set_configuration_parameters(time_scale=5,quality_level=0)\n",
    "\n",
    "#origin_env = UnityEnvironment(file_name=\"C://Users//raman//Documents//Pengsong//MIE1075_Soccer//buildmysoccer//SoccerTwos\", seed=1, side_channels=[engine_channel])\n",
    "origin_env = UnityEnvironment(file_name=\"C://Users//raman//Documents//Pengsong//MIE1075_Soccer//osoccer//UnityEnvironment\", seed=1, no_graphics=False,side_channels=[engine_channel])\n",
    "env = TransUnity2Gym(origin_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "step() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     obs_n, reward_n, done_n, _ \u001b[39m=\u001b[39m origin_env\u001b[39m.\u001b[39;49mstep([np\u001b[39m.\u001b[39;49marray([\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m]),np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m, \u001b[39m3\u001b[39;49m, size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m),np\u001b[39m.\u001b[39;49marray([\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m]),np\u001b[39m.\u001b[39;49marray([\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m]),np\u001b[39m.\u001b[39;49marray([\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m]),np\u001b[39m.\u001b[39;49marray([\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m]),np\u001b[39m.\u001b[39;49marray([\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m]),np\u001b[39m.\u001b[39;49marray([\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m]),np\u001b[39m.\u001b[39;49marray([\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m]),np\u001b[39m.\u001b[39;49marray([\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m])])\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(reward_n)\n",
      "File \u001b[1;32mc:\\Users\\raman\\.conda\\envs\\pengsong\\lib\\site-packages\\mlagents_envs\\timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    304\u001b[0m     \u001b[39mwith\u001b[39;00m hierarchical_timer(func\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: step() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    obs_n, reward_n, done_n, _ = origin_env.step([np.array([0,0,0]),np.random.randint(0, 3, size=3),np.array([0,0,0]),np.array([0,0,0]),np.array([0,0,0]),np.array([0,0,0]),np.array([0,0,0]),np.array([0,0,0]),np.array([0,0,0]),np.array([0,0,0])])\n",
    "    print(reward_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransUnity2Gym():\n",
    "    def __init__(self,env):\n",
    "        env.reset()\n",
    "        self.environment  = env\n",
    "        self.calculate_agent_num()\n",
    "        \n",
    "    def calculate_agent_num(self):\n",
    "        agent_sum = 0\n",
    "        behavior_names = list(self.environment.behavior_specs)\n",
    "        for behavior_name in behavior_names:\n",
    "            decision_steps, terminal_steps = self.environment.get_steps(behavior_name)\n",
    "            agent_sum += len(decision_steps)\n",
    "        self.n = agent_sum\n",
    "        print('self.agent_sum = ',self.n)\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        env = self.environment\n",
    "        env.reset()\n",
    "        behavior_names = list(env.behavior_specs)\n",
    "        obs_n=[0 for _ in range(self.n)]\n",
    "        \n",
    "        for behavior_name in behavior_names:\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            \n",
    "            if len(terminal_steps.agent_id) > 0:\n",
    "                for agent_id_terminated in terminal_steps:\n",
    "                    obs  = terminal_steps[agent_id_terminated].obs\n",
    "                    obs_s = np.concatenate([obs[0],obs[1]])\n",
    "                    obs_n[agent_id_terminated] = obs_s\n",
    "\n",
    "                \n",
    "            if len(decision_steps.agent_id) > 0:\n",
    "                for agent_id_decision in decision_steps:\n",
    "                     \n",
    "                    obs  = decision_steps[agent_id_decision].obs\n",
    "                    obs_s = np.concatenate([obs[0],obs[1]])\n",
    "                    obs_n[agent_id_decision] = obs_s\n",
    "                \n",
    "                \n",
    "        return obs_n\n",
    "    \n",
    "    def step(self,action_n):\n",
    "        \n",
    "        env = self.environment\n",
    "        behavior_names = list(env.behavior_specs)   \n",
    "\n",
    "        next_obs_n = [0 for _ in range(self.n)]\n",
    "        reward_n = [0 for _ in range(self.n)]\n",
    "        done_n = [False for _ in range(self.n)]\n",
    "        info = [0 for _ in range(self.n)]        \n",
    "        \n",
    "        for behavior_name in behavior_names:\n",
    "            \n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            print('pp_terminal_steps.agent_id',terminal_steps.agent_id)\n",
    "            print('pp_decision_steps.agent_id',decision_steps.agent_id)\n",
    "            \n",
    "            if(len(terminal_steps.agent_id)==0):\n",
    "                action = []\n",
    "                for i_d in decision_steps.agent_id:\n",
    "                    action.append(action_n[i_d])\n",
    "\n",
    "                action = np.array(action)\n",
    "                action_tuple = ActionTuple()\n",
    "                action_tuple.add_discrete(action)\n",
    "                env.set_actions(behavior_name,action_tuple)\n",
    "                env.step()\n",
    "            else:\n",
    "                for agent_id_terminated in terminal_steps:\n",
    "                    \n",
    "                    done = terminal_steps[agent_id_terminated].interrupted\n",
    "                    obs  = terminal_steps[agent_id_terminated].obs\n",
    "                    reward = terminal_steps[agent_id_terminated].reward\n",
    "                    obs_s = np.concatenate([obs[0],obs[1]])\n",
    "                    \n",
    "                    done_n[agent_id_terminated] = done\n",
    "                    next_obs_n[agent_id_terminated] = obs_s\n",
    "                    reward_n[agent_id_terminated] = reward\n",
    "                return next_obs_n, reward_n, done_n, info\n",
    "\n",
    "\n",
    "            \n",
    "        for behavior_name in behavior_names:\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            \n",
    "            #print('decision_steps.agent_id',decision_steps.agent_id,len(decision_steps.agent_id))\n",
    "            #print('terminal_steps.agent_id',terminal_steps.agent_id,len(terminal_steps.agent_id))\n",
    "\n",
    "            #print('decision_steps.obs',decision_steps.obs[0].shape)        \n",
    "            #print('terminal_steps.obs',terminal_steps.obs[0].shape)\n",
    "            \n",
    "            #print('decision_steps.reward',decision_steps.reward)        \n",
    "            #print('terminal_steps.reward',terminal_steps.reward)\n",
    "            \n",
    "            obs_s = np.concatenate([decision_steps.obs[0],decision_steps.obs[1]],axis=1)\n",
    "            #print(obs_s.shape)\n",
    "            #print('decision_steps.reward',decision_steps.reward)\n",
    "            \n",
    "            #print('terminal_steps.reward',terminal_steps.reward)\n",
    "            #print('terminal_steps.interrupted=',terminal_steps.interrupted) \n",
    "            \n",
    "            local_done = False\n",
    "            \n",
    "            if len(terminal_steps.agent_id) > 0:\n",
    "                for agent_id_terminated in terminal_steps:\n",
    "                    \n",
    "                    done = terminal_steps[agent_id_terminated].interrupted\n",
    "                    obs  = terminal_steps[agent_id_terminated].obs\n",
    "                    reward = terminal_steps[agent_id_terminated].reward\n",
    "                    obs_s = np.concatenate([obs[0],obs[1]])\n",
    "                    \n",
    "                    done_n[agent_id_terminated] = done\n",
    "                    next_obs_n[agent_id_terminated] = obs_s\n",
    "                    reward_n[agent_id_terminated] = reward\n",
    "                    \n",
    "                    print('dddone=',done,'agent_id_terminated=',agent_id_terminated) \n",
    "                \n",
    "            if len(decision_steps.agent_id) > 0:\n",
    "                print('decision_steps.agent_id=',decision_steps.agent_id)\n",
    "                for agent_id_decision in decision_steps:\n",
    "                     \n",
    "                    obs  = decision_steps[agent_id_decision].obs\n",
    "                    reward = decision_steps[agent_id_decision].reward\n",
    "                    obs_s = np.concatenate([obs[0],obs[1]])\n",
    "                    \n",
    "                    next_obs_n[agent_id_decision] = obs_s\n",
    "                    reward_n[agent_id_decision] = reward\n",
    "                    \n",
    "                    #print('agent_id_decision=',agent_id_decision) \n",
    "\n",
    "                \n",
    "\n",
    "                \n",
    "\n",
    "        if done_n[0] == True:  \n",
    "            print(len(next_obs_n),next_obs_n[0].shape,reward_n,done_n)\n",
    "        return next_obs_n, reward_n, done_n, info\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransUnity2Gym(envunity)\n",
    "\n",
    "\n",
    "for j in range(20):\n",
    "    env.reset()\n",
    "    print('epoch-----',j)\n",
    "    for i in range(5000):\n",
    "        actions = [np.random.randint(0, 3, size=3),np.random.randint(0, 3, size=3),np.random.randint(0, 3, size=3),np.random.randint(0, 3, size=3)]\n",
    "        print('step-----',i)\n",
    "        next_obs_n, reward_n, done_n, info = env.step(actions)\n",
    "        if done_n[0] ==True:\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'envunity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m TransUnity2Gym(\u001b[43menvunity\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'envunity' is not defined"
     ]
    }
   ],
   "source": [
    "env = TransUnity2Gym(envunity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.25487724, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.15981403, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.1623513 , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.6123384 , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.49686074, 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.4488938 , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.44361296,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.36697894, 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.25487724, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.15981403,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.1623513 , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.6123384 , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.49686074, 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.4488938 ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.44361296, 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.36697894, 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.25487724, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.15981403, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.1623513 , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.6123384 , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.49686074,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.4488938 , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.44361296, 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.36697894, 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.48487353, 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.3651251 ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.46994   , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.48487353, 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.3651251 , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.46994   , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.48487353,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.3651251 , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.46994   ], dtype=float32),\n",
       " array([1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.14001487, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.8332185 , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11123308, 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.93017876, 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.11147828,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.65755725, 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.14459884, 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.46950927, 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.48504525, 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.3779431 ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.43732482, 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.14001487, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.8332185 , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.11123308, 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.93017876,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.11147828, 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.65755725, 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.14459884, 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.46950927, 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.48504525,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.3779431 , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.43732482, 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.14001487, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.8332185 , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.11123308,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.93017876, 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.11147828, 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.65755725, 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.14459884, 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.46950927,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.48504525, 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.3779431 , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.43732482, 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.6499596 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.15698853,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.35542566, 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.6499596 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.15698853, 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.35542566, 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.6499596 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.15698853, 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.35542566], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.67603713, 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.87406474, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.55730516,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.16527824, 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.4226192 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.1576479 , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.35277972, 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.5166447 ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.3145469 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.67603713, 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.87406474, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.55730516, 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.16527824, 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.4226192 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.1576479 , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.35277972,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.5166447 , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.3145469 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.67603713, 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.87406474,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.55730516, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.16527824, 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.4226192 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.1576479 ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.35277972, 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.5166447 , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.3145469 , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.18583867, 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.20079505,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.30207568, 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.18583867, 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.20079505, 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.30207568, 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18583867,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.20079505, 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.30207568], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.8219182 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.6718299 , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.6854189 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.4817906 , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.71672666, 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.38439277, 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.5617753 , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.3318155 ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.47913098, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.8219182 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.6718299 , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.6854189 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.4817906 , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.71672666, 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.38439277, 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.5617753 ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.3318155 , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.47913098, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.8219182 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.6718299 ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.6854189 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.4817906 , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.71672666, 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.38439277,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.5617753 , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.3318155 , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.47913098, 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.34369886, 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.16726767,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.29555753, 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.34369886, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.16726767, 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.29555753, 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.34369886,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.16726767, 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.29555753], dtype=float32)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [0 2]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [1 3]\n",
      "decision_steps.agent_id= [0 2]\n",
      "decision_steps.agent_id= [1 3]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [0 2]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [1 3]\n",
      "decision_steps.agent_id= [0 2]\n",
      "decision_steps.agent_id= [1 3]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [0 2]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [1 3]\n",
      "decision_steps.agent_id= [0 2]\n",
      "decision_steps.agent_id= [1 3]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [0 2]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [1 3]\n",
      "decision_steps.agent_id= [0 2]\n",
      "decision_steps.agent_id= [1 3]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [0 2]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [1 3]\n",
      "decision_steps.agent_id= [0 2]\n",
      "decision_steps.agent_id= [1 3]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [0 2]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [1 3]\n",
      "decision_steps.agent_id= [0 2]\n",
      "decision_steps.agent_id= [1 3]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [0 2]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [1 3]\n",
      "decision_steps.agent_id= [0 2]\n",
      "decision_steps.agent_id= [1 3]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [0 2]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [1 3]\n",
      "decision_steps.agent_id= [0 2]\n",
      "decision_steps.agent_id= [1 3]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [0 2]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [1 3]\n",
      "decision_steps.agent_id= [0 2]\n",
      "decision_steps.agent_id= [1 3]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [0 2]\n",
      "pp_terminal_steps.agent_id []\n",
      "pp_decision_steps.agent_id [1 3]\n",
      "decision_steps.agent_id= [0 2]\n",
      "decision_steps.agent_id= [1 3]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10): \n",
    "    actions = [np.array([0,0,0]),np.random.randint(0, 3, size=3),np.array([0,0,0]),np.array([0,0,0])]\n",
    "    env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envunity\n",
    "# We will only consider the first Behavior\n",
    "behavior_names = list(env.behavior_specs)\n",
    "\n",
    "for behavior_name in behavior_names:\n",
    "\n",
    "    print(f\"\\nName of the behavior : {behavior_name}\")\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "\n",
    "\n",
    "    # Examine the number of observations per Agent\n",
    "    print(\"Number of observations : \", len(spec.observation_specs))\n",
    "    print('\\n')\n",
    "    for spec1 in spec.observation_specs:\n",
    "        print(spec1)\n",
    "    print('\\n')\n",
    "\n",
    "    # Is there a visual observation ?\n",
    "    # Visual observation have 3 dimensions: Height, Width and number of channels\n",
    "    vis_obs = any(len(spec.shape) == 3 for spec in spec.observation_specs)\n",
    "    print(\"Is there a visual observation ?\", vis_obs)\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "    # Is the Action continuous or multi-discrete ?\n",
    "    if spec.action_spec.continuous_size > 0:\n",
    "      print(f\"There are {spec.action_spec.continuous_size} continuous actions\")\n",
    "    if spec.action_spec.is_discrete():\n",
    "      print(f\"There are {spec.action_spec.discrete_size} discrete actions\")\n",
    "    print(spec.action_spec)\n",
    "\n",
    "\n",
    "    # How many actions are possible ?\n",
    "    #print(f\"There are {spec.action_size} action(s)\")\n",
    "\n",
    "    # For discrete actions only : How many different options does each action has ?\n",
    "    if spec.action_spec.discrete_size > 0:\n",
    "      for action, branch_size in enumerate(spec.action_spec.discrete_branches):\n",
    "        print(f\"Action number {action} has {branch_size} different options\")\n",
    "        \n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    print('\\n')\n",
    "    print('len decision_steps', len(decision_steps))\n",
    "    print('decision_steps.obs', decision_steps.obs[0].shape,decision_steps.obs[1].shape)\n",
    "    print(np.concatenate([decision_steps.obs[0],decision_steps.obs[1]],axis=1).shape)\n",
    "    print('decision_steps.rewards, ', decision_steps.reward)\n",
    "    print('terminal_steps.obs, ', terminal_steps.obs)\n",
    "    \n",
    "    print('\\n')\n",
    "    for agent_id_terminated in decision_steps:\n",
    "            # Create its last experience (is last because the Agent terminated)\n",
    "\n",
    "        print(decision_steps[agent_id_terminated].reward)\n",
    "        #print(decision_steps[agent_id_terminated].obs[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_actions(behavior_name, spec.action_spec.empty_action(len(decision_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for index, obs_spec in enumerate(spec.observation_specs):\n",
    "    if len(obs_spec.shape) == 3:\n",
    "        print(\"Here is the first visual observation\")\n",
    "        plt.imshow(decision_steps.obs[index][0,:,:,:])\n",
    "        plt.show()\n",
    "\n",
    "for index, obs_spec in enumerate(spec.observation_specs):\n",
    "    if len(obs_spec.shape) == 1:\n",
    "        print(\"First vector observations : \", decision_steps.obs[index][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "for episode in range(1):\n",
    "    env.reset()\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    tracked_agent = -1 # -1 indicates not yet tracking\n",
    "    done = False # For the tracked_agent\n",
    "    episode_rewards = 0 # For the tracked_agent\n",
    "    \n",
    "    dict_last_action_from_agent: Dict[int, np.ndarray] = {}\n",
    "    \n",
    "    while not done:\n",
    "        # Track the first agent we see if not tracking\n",
    "        # Note : len(decision_steps) = [number of agents that requested a decision]\n",
    "        if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "            tracked_agent = decision_steps.agent_id[0]\n",
    "            print(decision_steps.agent_id)\n",
    "\n",
    "        # Generate an action for all agents\n",
    "        action = spec.action_spec.random_action(len(decision_steps))\n",
    "        #print('len(decision_steps)',len(decision_steps))\n",
    "        #print('action',action.discrete)\n",
    "\n",
    "        # Set the actions\n",
    "        env.set_actions(behavior_name, action)\n",
    "\n",
    "        # Move the simulation forward\n",
    "        env.step()\n",
    "\n",
    "        # Get the new simulation results\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        print(decision_steps[1])\n",
    "        if tracked_agent in decision_steps: # The agent requested a decision\n",
    "            episode_rewards += decision_steps[tracked_agent].reward\n",
    "        if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "            episode_rewards += terminal_steps[tracked_agent].reward\n",
    "            done = True\n",
    "        print('episode_rewards = ',episode_rewards)\n",
    "    print(f\"Total rewards for episode {episode} is {episode_rewards}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "while True:\n",
    "    if count > 500000:\n",
    "        break\n",
    "    for name in behavior_names:\n",
    "        states = env.get_steps(name)\n",
    "        \n",
    "        # 在此添加算法\n",
    "        actions = ActionTuple()\n",
    "        \n",
    "        # 测试时让4个agent向四个方向随机移动 使用中应改为算法提供的action\n",
    "        ac = np.random.randint(0, 5, size=48).reshape(-1, 3)\n",
    "        actions.add_discrete(ac)\n",
    "\n",
    "        env.set_actions(name, actions)\n",
    "        \n",
    "        DecisionSteps, TerminalSteps = env.get_steps(name)\n",
    "        print(DecisionSteps,TerminalSteps)\n",
    "        \n",
    "    count += 1\n",
    "    \n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Soccer.app\"`\n",
    "- **Windows** (x86): `\"path/to/Soccer_Windows_x86/Soccer.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Soccer_Windows_x86_64/Soccer.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Soccer_Linux/Soccer.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Soccer_Linux/Soccer.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Soccer_Linux_NoVis/Soccer.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Soccer_Linux_NoVis/Soccer.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Soccer.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Soccer.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"C://Users//raman//Documents//Pengsong//MIE1075_Soccer//buildmysoccer//SoccerTwos_BurstDebugInformation_DoNotShip//UnityEnvironment\")\n",
    "#C://Users//raman//Documents//Pengsong//MIE1075_Soccer//buildmysoccer//UnityEnvironment\n",
    "#C://Users//raman//Downloads//Soccer_Windows_x86_64//Soccer_Windows_x86_64//Soccer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"C://Users//raman//Documents//Pengsong//MIE1075_Soccer//buildmysoccer//SoccerTwos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the brain names\n",
    "print(env.brain_names)\n",
    "\n",
    "# set the goalie brain\n",
    "g_brain_name = env.brain_names[0]\n",
    "g_brain = env.brains[g_brain_name]\n",
    "\n",
    "# set the striker brain\n",
    "s_brain_name = env.brain_names[1]\n",
    "s_brain = env.brains[s_brain_name]\n",
    "\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)\n",
    "\n",
    "# number of agents \n",
    "num_g_agents = len(env_info[g_brain_name].agents)\n",
    "print('Number of goalie agents:', num_g_agents)\n",
    "num_s_agents = len(env_info[s_brain_name].agents)\n",
    "print('Number of striker agents:', num_s_agents)\n",
    "\n",
    "# number of actions\n",
    "g_action_size = g_brain.vector_action_space_size\n",
    "print('Number of goalie actions:', g_action_size)\n",
    "s_action_size = s_brain.vector_action_space_size\n",
    "print('Number of striker actions:', s_action_size)\n",
    "\n",
    "# examine the state space \n",
    "g_states = env_info[g_brain_name].vector_observations\n",
    "g_state_size = g_states.shape[1]\n",
    "print('There are {} goalie agents. Each receives a state with length: {}'.format(g_states.shape[0], g_state_size))\n",
    "s_states = env_info[s_brain_name].vector_observations\n",
    "s_state_size = s_states.shape[1]\n",
    "print('There are {} striker agents. Each receives a state with length: {}'.format(s_states.shape[0], s_state_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0):                                         # play game for 2 episodes\n",
    "    env_info = env.reset(train_mode=False)                 # reset the environment    \n",
    "    g_states = env_info[g_brain_name].vector_observations  # get initial state (goalies)\n",
    "    s_states = env_info[s_brain_name].vector_observations  # get initial state (strikers)\n",
    "    g_scores = np.zeros(num_g_agents)                      # initialize the score (goalies)\n",
    "    s_scores = np.zeros(num_s_agents)                      # initialize the score (strikers)\n",
    "    while True:\n",
    "        # select actions and send to environment\n",
    "        g_actions = np.random.randint(g_action_size, size=num_g_agents)\n",
    "        s_actions = np.random.randint(s_action_size, size=num_s_agents)\n",
    "\n",
    "        actions = dict(zip([g_brain_name, s_brain_name], \n",
    "                           [g_actions, s_actions]))\n",
    "        print(actions)\n",
    "        env_info = env.step(actions)                       \n",
    "        \n",
    "        # get next states\n",
    "        g_next_states = env_info[g_brain_name].vector_observations         \n",
    "        s_next_states = env_info[s_brain_name].vector_observations  \n",
    "        \n",
    "        # get reward and update scores\n",
    "        g_rewards = env_info[g_brain_name].rewards  \n",
    "        s_rewards = env_info[s_brain_name].rewards\n",
    "        g_scores += g_rewards\n",
    "        s_scores += s_rewards\n",
    "        \n",
    "        # check if episode finished\n",
    "        done = np.any(env_info[g_brain_name].local_done)  \n",
    "        \n",
    "        # roll over states to next time step\n",
    "        g_states = g_next_states\n",
    "        s_states = s_next_states\n",
    "        \n",
    "        # exit loop if episode finished\n",
    "        if done:                                           \n",
    "            break\n",
    "    print('Scores from episode {}: {} (goalies), {} (strikers)'.format(i+1, g_scores, s_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "from maddpg import MADDPG\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "\n",
    "# from maddpg import MADDPG\n",
    "# from buffer import ReplayBuffer\n",
    "# from utilities import transpose_list, transpose_to_tensor\n",
    "# from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_maddpg = MADDPG(336, 4, 2, 1976)\n",
    "g_agent = Agent(336,4,2,1976)\n",
    "\n",
    "s_maddpg = MADDPG(336, 6, 2, 1976)\n",
    "s_agent = Agent(336,6,2,1976)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_scores_max_hist = []\n",
    "g_scores_mean_hist = []\n",
    "\n",
    "s_scores_max_hist = []\n",
    "s_scores_mean_hist = []\n",
    "\n",
    "def maddpg_train(n_episodes=3000):\n",
    "    \n",
    "    g_scores_deque = deque(maxlen=100)\n",
    "    s_scores_deque = deque(maxlen=100)\n",
    "\n",
    "    \n",
    "    g_solved = False\n",
    "    s_solved = False\n",
    "    \n",
    "    for i_episode in range(n_episodes):\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)                 # reset the environment   \n",
    "        g_states = env_info[g_brain_name].vector_observations\n",
    "        s_states = env_info[s_brain_name].vector_observations\n",
    "\n",
    "        \n",
    "        g_scores = np.zeros(num_g_agents)\n",
    "        s_scores = np.zeros(num_s_agents)\n",
    "        \n",
    "        g_maddpg.reset()\n",
    "        s_maddpg.reset()\n",
    "        \n",
    "        step = 0\n",
    "        \n",
    "        while True:\n",
    "            step += 1\n",
    "            g_actions = g_maddpg.act(g_states, i_episode, add_noise=False)\n",
    "            s_actions = s_maddpg.act(s_states, i_episode, add_noise=False)\n",
    "\n",
    "            g_actions = np.argmax(g_actions,1)\n",
    "            s_actions = np.argmax(s_actions,1)\n",
    "                                 \n",
    "            actions = dict(zip([g_brain_name, s_brain_name], \n",
    "                               [g_actions, s_actions]))\n",
    "            \n",
    "            env_info = env.step(actions) \n",
    "            \n",
    "            # get next states\n",
    "            g_next_states = env_info[g_brain_name].vector_observations         \n",
    "            s_next_states = env_info[s_brain_name].vector_observations\n",
    "            \n",
    "            # get reward and update scores\n",
    "            g_rewards = env_info[g_brain_name].rewards  \n",
    "            s_rewards = env_info[s_brain_name].rewards\n",
    "            g_scores += g_rewards\n",
    "            s_scores += s_rewards\n",
    "\n",
    "            done = env_info[g_brain_name].local_done\n",
    "            \n",
    "            \n",
    "            g_maddpg.step(i_episode, g_states, g_actions, g_rewards, g_next_states, done)\n",
    "            s_maddpg.step(i_episode, s_states, s_actions, s_rewards, s_next_states, done)\n",
    "            \n",
    "            if np.any(done):\n",
    "                break\n",
    "                \n",
    "             # roll over states to next time step\n",
    "            g_states = g_next_states\n",
    "            s_states = s_next_states\n",
    "            \n",
    "        g_score_max = np.max(g_scores)\n",
    "        g_scores_deque.append(g_score_max)\n",
    "        g_score_mean = np.mean(g_scores_deque)\n",
    "        g_scores_max_hist.append(g_score_max)\n",
    "        g_scores_mean_hist.append(g_score_mean)\n",
    "        \n",
    "        s_score_max = np.max(s_scores)\n",
    "        s_scores_deque.append(s_score_max)\n",
    "        s_score_mean = np.mean(s_scores_deque)\n",
    "        s_scores_max_hist.append(s_score_max)\n",
    "        s_scores_mean_hist.append(s_score_mean)\n",
    "\n",
    "        print('\\r{} episode\\tavg g_score {:.5f}\\tmax score {:.5f}'.format(i_episode, np.mean(g_scores_deque), g_score_max), end='')\n",
    "        if g_solved == False and g_score_mean >= 0.5:\n",
    "            print('\\nEnvironment g_solved after {} episodes with the average score {}\\n'.format(i_episode, g_score_mean))\n",
    "            g_maddpg.save(\"g_\")\n",
    "            g_solved = True\n",
    "            \n",
    "        print('\\r{} episode\\tavg s_score {:.5f}\\tmax score {:.5f}'.format(i_episode, np.mean(s_scores_deque), s_score_max), end='')\n",
    "        if s_solved == False and s_score_mean >= 0.5:\n",
    "            print('\\nEnvironment g_solved after {} episodes with the average score {}\\n'.format(i_episode, s_score_mean))\n",
    "            s_maddpg.save(\"s_\")\n",
    "            s_solved = True  \n",
    "            \n",
    "        if i_episode % 5 == 0:\n",
    "            print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maddpg_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('checkpoint_agent0_actor.pth', map_location='cpu'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic0_critic.pth', map_location='cpu'))\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_agent1_actor.pth', map_location='cpu'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic1_critic.pth', map_location='cpu'))\n",
    "\n",
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = agent.act(states,i, add_noise= False)                      # select actions from loaded model agent\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):                                         # play game for 2 episodes\n",
    "    env_info = env.reset(train_mode=False)                 # reset the environment    \n",
    "    g_states = env_info[g_brain_name].vector_observations  # get initial state (goalies)\n",
    "    s_states = env_info[s_brain_name].vector_observations  # get initial state (strikers)\n",
    "    g_scores = np.zeros(num_g_agents)                      # initialize the score (goalies)\n",
    "    s_scores = np.zeros(num_s_agents)                      # initialize the score (strikers)\n",
    "    while True:\n",
    "        # select actions and send to environment\n",
    "        g_actions = np.random.randint(g_action_size, size=num_g_agents)\n",
    "        s_actions = np.random.randint(s_action_size, size=num_s_agents)\n",
    "        print(g_actions,s_actions)\n",
    "        actions = dict(zip([g_brain_name, s_brain_name], \n",
    "                           [g_actions, s_actions]))\n",
    "        print(actions)\n",
    "        env_info = env.step(actions)                       \n",
    "        \n",
    "        # get next states\n",
    "        g_next_states = env_info[g_brain_name].vector_observations         \n",
    "        s_next_states = env_info[s_brain_name].vector_observations\n",
    "        \n",
    "        # get reward and update scores\n",
    "        g_rewards = env_info[g_brain_name].rewards  \n",
    "        s_rewards = env_info[s_brain_name].rewards\n",
    "        g_scores += g_rewards\n",
    "        s_scores += s_rewards\n",
    "        \n",
    "        # check if episode finished\n",
    "        done = np.any(env_info[g_brain_name].local_done)  \n",
    "        \n",
    "        # roll over states to next time step\n",
    "        g_states = g_next_states\n",
    "        s_states = s_next_states\n",
    "        \n",
    "        # exit loop if episode finished\n",
    "        if done:                                           \n",
    "            break\n",
    "    print('Scores from episode {}: {} (goalies), {} (strikers)'.format(i+1, g_scores, s_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pengsong')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0705781b6dfe31907bad64ee629835fcb3aa43a9e88e5ba2baa2ae17469aa9ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
